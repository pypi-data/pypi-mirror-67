{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Welcome to the Musical Gestures Toolbox (Python) - Tutorial/Documentation"}, {"cell_type": "markdown", "metadata": {}, "source": "## Video visualisation\nVideos can be watched as they are, but they can also be used to develop new visualisations to be used for analysis. The aim of creating such alternate displays from video recordings is to uncover features, structures and similarities within the material itself, and in relation to, for example, score material. Three useful visualisation techniques here are motion images, motion history images and motiongrams.\n\nMGT can generate both dynamic and static visualizations, as well as some quantitative data:\n\n- dynamic visualisations (video files)\n    - motion video\n    - motion history video\n- static visualisations (images)\n    - motion average image\n    - motiongrams\n    - videograms\n- motion data (csv files)\n    - quantity of motion\n    - centroid of motion\n    - area of motion\n\nIn the following we will try this ourselves, and look at the different types."}, {"cell_type": "markdown", "metadata": {}, "source": "## Dependencies\n\nIf you installed musicalgestures via pip (\"`pip install musicalgestures`\") you should have all dependencies installed. To make sure you have all the necessary dependencies, evaluate the following line in the terminal:\n\n`pip3 install numpy pandas matplotlib opencv-python moviepy ffmpeg ffmpeg-python scipy`\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Using Google Colab\nIn case you are using this notebook in Google Colab, execute the following cell to install `musicalgestures`:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install musicalgestures"}, {"cell_type": "markdown", "metadata": {}, "source": "## Import\n\nIf you have all the dependencies installed, go ahead and import the `musicalgestures`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import musicalgestures"}, {"cell_type": "markdown", "metadata": {}, "source": "## The MgObject\n\n### Simple video import\n\nNow we create our mg (musical gestures) object. \n\nYou can simply read a video file from the current directory this way:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi') # from the current directory"}, {"cell_type": "markdown", "metadata": {}, "source": "Relative paths also work. Here is _pianist.avi_ from the __examples__ (sibling) directory:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('./examples/pianist.avi') # from a sibling directory"}, {"cell_type": "markdown", "metadata": {}, "source": "With absolute path:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os\nabs_path = os.path.abspath('dance.avi')\nprint(f\"The absolute path to dance.avi is '{abs_path}'.\")\nmg = musicalgestures.MgObject(abs_path) # as absolute path"}, {"cell_type": "markdown", "metadata": {}, "source": "You can watch your video with calling the `show()` method:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.show() # press q to quit video"}, {"cell_type": "markdown", "metadata": {}, "source": "### A brief note on containers and codecs\nWe aim to provide `musicalgestures` as a cross-platform, flexible toolbox. Unfortunately different video containers and codecs are not equally well-supported across different containers, API-s, and operating systems. Our current solution to ensure that our toolbox works for you regardless of all that is to use the `mjpeg` codec with the `avi` container. Therefore all imported videos which don't match these are automatically converted (using ffmpeg) as a 0th step, and all subsequent processes are executed on the resulting files."}, {"cell_type": "markdown", "metadata": {}, "source": "## Preprocessing modules\n### Trimming\nWhen creating the object you can also already apply some preprocessing. For example you can trim the duration of the video like this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# the numbers used for starttime and endtime represent time from the video's timeline in seconds\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15)\nmg.show() # view the result, press q to quit video"}, {"cell_type": "markdown", "metadata": {}, "source": "This will create the file *dance_trim.avi* in the same directory."}, {"cell_type": "markdown", "metadata": {}, "source": "### Skipping\nIn order to save time, skipping every other frame, or more, in the analysis can give you a faster analysis while still getting an idea of the motion. You can for example set this by adding `skip=2`, to skip two frames before including a frame in the analysis, then skipping two again. "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=2)\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "This will create the file *dance_trim_skip.avi* in the same directory. Notice how the added suffixes at the end of the file's name can inform you about the processes the material went through. "}, {"cell_type": "markdown", "metadata": {}, "source": "### It's a chain\nIt is also worth to note that the preprocessing modules work as a chain (- more on that below). In this case that means we first load the video file, then trim its start to 5s and its end to 15s. Then we take the resulting *dance_trim.avi* and discard 2 out of every 3 frames. (Keeping the 1st, skipping 2nd and 3rd, keeping the 4th, skipping 5th and 6th, and so on...) The resulting file of this process is *dance_trim_skip.avi*."}, {"cell_type": "markdown", "metadata": {}, "source": "### Rotating\n\nSometimes source videos are recorded with a slightly off horizon, or with the camera mounted sideways, therefore it is desirable to rotate the video by a few (or more) degrees. We can do this by simply specifying the angle we want to rotate with for the `rotate` parameter of our MgObject:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# rotate by 90 degrees\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90)\nmg.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# or just a little bit...\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=5.31)\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Again, the resulting filename *dance_trim_skip_rot.avi* will inform us about the chain of processes *dance.avi* went through."}, {"cell_type": "markdown", "metadata": {}, "source": "### Adjusting contrast and brightness\nDuring preprocessing you can also add (or remove) some contrast and brightness of your video.\n\nLet's crank up the contrast and brighten up our *dance.avi*!"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90, contrast=100, brightness=20)\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "The resulting file is now called *dance_trim_skip_rot_cb.avi*."}, {"cell_type": "markdown", "metadata": {}, "source": "### Cropping\n\nIf the video frame has big areas with no motion occuring, a lot of time could be saved if only the area with motion was used in the analysis. One useful tool developed for the pre-analysis is the crop = 'auto' input, which automatically finds the area with significant motion in the input video. The movement occuring has to be above a low threshold, as to not include irrelevant background motion from shadows, dust etc. Another mode is crop = 'manual', where you can manually mark a rectangle around your area of interest."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Automatic cropping"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90, contrast=100, brightness=20, crop='auto')\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Manual cropping"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90, contrast=100, brightness=20, crop='manual')\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "The resulting file is now called *dance_trim_skip_rot_cb_crop.avi*."}, {"cell_type": "markdown", "metadata": {}, "source": "### Grayscale mode\n\nSo far all of our work preserved the color space of the source video. At the final preprocessing stage we can also choose to convert the video to grayscale with specifying `color=False` (by default `color=True`). This will not only result in a grayscale version of our source video, but also informs all future processes to work in grayscale mode. The technical benefit of this can be slightly shorter processing times at most processes, since in grayscale mode we process a single color channel per frame (instead of three channels). Try this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90, contrast=100, brightness=20, crop='auto', color=False)\nmg.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "The resulting file is now called *dance_trim_skip_rot_cb_crop_gray.avi*."}, {"cell_type": "markdown", "metadata": {}, "source": "### Summary of preprocessing modules\nAs we have seen, we can optionally apply six types of preprocessing to the video we load into our MgObject, they are (in order of execution):\n\n- trim: Trim contents of the video based on `starttime` and `endtime`.\n- skip: Skip every n frames, where n is determined by `skip`.\n- rotate: Rotate the video by an angle determined by `rotate`.\n- cb: Adjust contrast and brightness of the video, where `contrast` and `brightness` are the level of adjustment in percentages (meaning `contrast=0` will not apply any change). both values range from `-100` to `100`.\n- crop: Crop frames in video. If `crop='auto'` the module will attempt to find the area of motion, if `crop='manual'` we can draw the cropping rectangle over the first frame.\n- grayscale: Convert the video to grayscale with specifying `color=False`. This will also cause all further processes called on the MgObject to function in grayscale mode.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Keep everything\nNotice that although we can optionally apply up to four preprocessing modules to our source video, normally we only keep the final result. If you would like to keep the results of all modules, set `keep_all=True`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3, rotate=90, contrast=100, brightness=20, crop='auto', color=False, keep_all=True)"}, {"cell_type": "markdown", "metadata": {}, "source": "This will output six new video files:\n- *dance_trim.avi*\n- *dance_trim_skip.avi*\n- *dance_trim_skip_rot.avi*\n- *dance_trim_skip_rot_cb.avi*\n- *dance_trim_skip_rot_cb_crop.avi*\n- *dance_trim_skip_rot_cb_crop_gray.avi*"}, {"cell_type": "markdown", "metadata": {}, "source": "## Processes\n\nIn the following we will take a look at several functions to further process our videos. These include:\n- `motion()`: The most frequently used function, generates a *_motion* video, horizontal and vertical motiongrams, and plots about the centroid and quantity of motion found in the video.\n- `history()`: Generates a *_history* video by layering the last n frames on the current frame for each frame in the video.\n- `average()`: Generates an *_average* image of all frames in the video.\n- `flow.sparse()`: Generates a *_sparse* optical flow video.\n- `flow.dense()`: Generates a *_dense* optical flow video. "}, {"cell_type": "markdown", "metadata": {}, "source": "### Motion analysis\n\nBy calling the `motion()` function, we will generate a number of files from the input video, in the same location as the source file.\n\nThese include:\n- *<input_filename>_motion.avi*: The motion video that is used as the source for the rest of the analysis.\n- *<input_filename>_mgx.png*: A horizontal motiongram.\n- *<input_filename>_mgy.png*: A vertical motiongram.\n- *<input_filename>_motion_com_qom.png*: An image file with plots of centroid and quantity of motion\n\nWe will examine each of these in a little more detail."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import musicalgestures\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=1, contrast=100, brightness=20, crop='auto')\nmg.motion()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ls # take a look at the output files"}, {"cell_type": "markdown", "metadata": {}, "source": "We can now look at the results with using the `key` parameter of `show()`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.show(key='motion') # show the motion video of the preprocessed input, in this case 'dance_trim_skip_cb_crop_motion.avi'"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.show(key='mgx') # show the horizontal motiongram, here 'dance_trim_skip_cb_crop_mgx.png'"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.show(key='mgy') # show the vertical motiongram"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.show(key='plot') # show the image of the two plots ('Centroid of motion' and 'Quantity of motion') also shown at the end of motion()"}, {"cell_type": "markdown", "metadata": {}, "source": "Alternatively we can display the images right here in our notebook:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from IPython.display import Image\nx = Image('dance_trim_skip_cb_crop_mgx.png')\nx"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "y = Image('dance_trim_skip_cb_crop_mgy.png')\ny"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "com_qom = Image('dance_trim_skip_cb_crop_motion_com_qom.png')\ncom_qom"}, {"cell_type": "markdown", "metadata": {}, "source": "You can also configure `motion()` to output only the files you need:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import musicalgestures\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=1, contrast=100, brightness=20, crop='auto')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# without plot\nmg.motion(save_plot=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# without plot and data\nmg.motion(save_plot=False, save_data=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# without plot, data and motiongrams (so only the video)\nmg.motion(save_plot=False, save_data=False, save_motiongrams=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# without video, plot and motiongrams (so only the data)\nmg.motion(save_plot=False, save_video=False, save_motiongrams=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "When it comes to the motion data, you can choose from several different formats:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import musicalgestures\nmg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=1, contrast=100, brightness=20, crop='auto')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# save only the data as .csv (default)\nmg.motion(save_plot=False, save_motiongrams=False, save_video=False, data_format=\"csv\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# save only the data as .tsv\nmg.motion(save_plot=False, save_motiongrams=False, save_video=False, data_format=\"tsv\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# save only the data as .txt\nmg.motion(save_plot=False, save_motiongrams=False, save_video=False, data_format=\"txt\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# saving in multiple formats if data_format is a list\nmg.motion(save_plot=False, save_motiongrams=False, save_video=False, data_format=[\"txt\", \"csv\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Filtering types\nIf you think there is too much noise in the output images or video, you may choose to use some other filter settings.\n\nFiltertypes available are:\n\n- `Regular` turns all values below `thresh` to 0.\n- `Binary` turns all values below `thresh` to 0, above `thresh` to 1.\n- `Blob` removes individual pixels with erosion method.\n\nTry this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion(filtertype='Blob')\nmg.show(key='motion')"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Effects of filtering\n\nFinding the right `thresh`old value is crucial for accurate motion extraction. Let's see a few examples.\n\nFirst we import the requirements for the rest of the code."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import musicalgestures\nfrom IPython.display import Image"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we import the example video."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=3, endtime=15, skip=0, contrast=100, brightness=20)"}, {"cell_type": "markdown", "metadata": {}, "source": "First we can try to run without any threshold. This will result in a result in which much of the background noise will be visible, including traces of keyframes if the video file has been compressed."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion(thresh=0)\nx = Image('dance_trim_cb_mgy.png')\nx"}, {"cell_type": "markdown", "metadata": {}, "source": "Adding just a little bit of thresholding (0.02 here) will drastically improve the final result."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion(thresh=0.02)\nx = Image('dance_trim_cb_mgy.png')\nx"}, {"cell_type": "markdown", "metadata": {}, "source": "The standard threshold value (0.1) generally works well for many types of videos."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion(thresh=0.1)\nx = Image('dance_trim_cb_mgy.png')\nx"}, {"cell_type": "markdown", "metadata": {}, "source": "A more extreme value (for example 0.5) will remove quite a lot of the content, but may be useful in some cases with very noisy videos."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion(thresh=0.5)\nx = Image('dance_trim_cb_mgy.png')\nx"}, {"cell_type": "markdown", "metadata": {}, "source": "As the above examples have shown, choosing the thresholding value is important for the final output result. While it often works to use the default value (0.1), you may improve the result by testing different thresholds."}, {"cell_type": "markdown", "metadata": {}, "source": "### History tracking\nAs we have seen above, `motion()` is useful if you want to remove the still content of your video, only keeping what is different in subsequent frames. Sometimes it is also useful to visualize changes between frames in a different way: layering the last n frames on top of the current one as a video delay. With `history()` you can achieve this, optionally setting the `history length` to the number of past frames you want to see on the current frame (ie. the length of the delay).\n\nTry this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.history()\nmg.show(key='history')"}, {"cell_type": "markdown", "metadata": {}, "source": "By default `history_length=10`. Let's increase it to 20!"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.history(history_length=20)\nmg.show(key='history')"}, {"cell_type": "markdown", "metadata": {}, "source": "### Motion history\n\nTo expressively visualize the trajectory of a moving content in a video, you can apply the history process on a motion video. You can do this by chaining `motion()` into `history()`. (More about chaining below!)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.motion().history()\nmg.show(key='motionhistory')"}, {"cell_type": "markdown", "metadata": {}, "source": "### Average image\nYou can also summarize the content of a video by showing the average of all frames in a single image."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.average()\nmg.show(key='average')"}, {"cell_type": "markdown", "metadata": {}, "source": "Embedded in the notebook:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from IPython.display import Image\naverage = Image('dance_trim_cb_average.png')\naverage"}, {"cell_type": "markdown", "metadata": {}, "source": "### Optical flow\nIt is also possible to track the direction certain points - or all points - move in a video, this is called 'optical flow'. It has two types: the *sparse optical flow*, which is for tracking a small (sparse) set of points, visualized with an overlay of dots and lines drawing the trajectory of the chosen points as they move in the video.  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.flow.sparse()\nmg.show(key='sparse')"}, {"cell_type": "markdown", "metadata": {}, "source": "Note that sparse optical flow usually works well with slow and continuous movements, where the points to be tracked are not occluded by other objects throughout the course of motion.\nWhere spare optical flow becomes less reliable, *dense optical flow* often yields more robust results. In dense optical flow the analysis attempts to track the movement of each pixel (or more precisely groups of pixels), colorcoding them with a unique color for each unique direction."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.flow.dense()\nmg.show(key='dense')"}, {"cell_type": "markdown", "metadata": {}, "source": "Sparse optical flow can get confused by too fast movement (ie. too big distance between the locations of a tracked point between two consequtive frames), so it is typically advised not to have a too high `skip` value in the preprocessing stage for it to work properly.\nDense optical flow on the other hand has issues with very slow movement, which sometimes gets below the treshold of what is considered 'a movement' resulting in a blinking video, where the more-or-less idle moments are rendered completely black. If your source video contains such moments, you can try setting `skip_empty=True`, which will discard all the (completely) black frames, eliminating the binking. "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.flow.dense(skip_empty=True)\nmg.show(key='dense')"}, {"cell_type": "markdown", "metadata": {}, "source": "## Chaining\n\nSo far our workflow consisted of the following steps:\n- 1. Creating an MgObject which loads a video file and optionally applies some preprocessing to it.\n- 2. Calling a process on the MgObject.\n- 3. Viewing the result.\n\nSomething like this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3)\nmg.motion()\nmg.show(key='motion')"}, {"cell_type": "markdown", "metadata": {}, "source": "This is convenient if you want to apply several different processes on the same input video."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg = musicalgestures.MgObject('dance.avi', starttime=5, endtime=15, skip=3)\nmg.motion()\nmg.history()\nmg.average()"}, {"cell_type": "markdown", "metadata": {}, "source": "The Musical Gestures Toolbox also offers an alternative workflow in case you want to apply a proccess on the result of a previous process. Although `show()` is not really a process (ie. it does not yield a file as a result) it can provide a good example of the use of chaining:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# this...\nmg.motion().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ...is the equivalent of this!\nmg.motion()\nmg.show(key='motion')"}, {"cell_type": "markdown", "metadata": {}, "source": "It also works with images:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "mg.average().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "But chaining can go further than this. How about reading (and preprocessing) a video, rendering its motion video, the motion history and the average of the motion history, with showing the *_motion_history_average.png* at the end - all as a one-liner?!"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "musicalgestures.MgObject('dance.avi', skip=4, crop='auto').motion().history().average().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# equivalent without chaining\nmg = musicalgestures.MgObject('dance.avi', skip=4, crop='auto')\nmm = mg.motion()\nmh = mm.history()\nmh.average()\nmh.show(key='average')"}, {"cell_type": "markdown", "metadata": {}, "source": "Some other examples:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# rendering and viewing the motion video \nmusicalgestures.MgObject('dance.avi', skip=4).motion().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# rendering the motion video, the motion history video, and viewing the latter\nmusicalgestures.MgObject('dance.avi', skip=3).motion().history().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# rendering the motion video, the motion average image, and viewing the latter\nmusicalgestures.MgObject('dance.avi', skip=15).motion().average().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Chaining can also save time (and space) when designing loops for processing a folder of videos. Here is an example:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os\nfrom musicalgestures import MgObject as Mg\n\nmy_videos_folder = 'C:/Users/User/Desktop/test-videos/'\n\nmy_videos = [my_videos_folder + video for video in os.listdir(my_videos_folder) if os.path.splitext(video)[1] in ['.avi', '.mp4', '.mov', '.mkv']]\n\nfor video in my_videos:\n    print(f'Processing {video}...')\n    Mg(video, skip=10).motion().history().average()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3.8.2 64-bit", "language": "python", "name": "python38264bit6c8361fb7b8545abb43250b0e650bd81"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}}, "nbformat": 4, "nbformat_minor": 2}