from .snowflake import getclient, SnowflakeClient
from google.cloud import storage
from google.auth.transport.requests import AuthorizedSession
from functools import lru_cache, wraps
import json
from collections import namedtuple
from socket import gethostname
from getpass import getuser
import time
import os
import calendar
import threading
import inspect
from contextlib import contextmanager
import warnings
import sys
import traceback
import tempfile
import subprocess

@lru_cache()
def _getblob_client(credentials):
    return storage.Client(credentials=credentials)

@lru_cache()
def _getblob_bucket(credentials, bucket_id, user_project):
    return _getblob_client(credentials).bucket(bucket_id, user_project)

def _bulk_upload(bucket, data):
    print("Hound executing batch upload of", len(data), "records")
    main_thread = threading.main_thread()
    notified = False
    try:
        with tempfile.TemporaryDirectory() as tempdir:
            for i, (path, obj) in enumerate(data):
                if i % 10 == 0 and not (notified or main_thread.is_alive()):
                    print("Hound is still updating", len(data) - i, "records in the background. Python will close when it's done")
                    notified = True
                fullpath = os.path.join(tempdir, path)
                os.makedirs(os.path.dirname(fullpath), exist_ok=True)
                with open(fullpath, 'w') as w:
                    json.dump(obj, w)
            if not (notified or main_thread.is_alive()):
                print("Hound is still updating records in the background. Python will close when it's done")
                notified = True
            proc = subprocess.Popen(
                'gsutil -m -h "Content-Type:text/plain" cp -r {}/hound gs://{}'.format(
                    tempdir,
                    bucket.name
                ),
                shell=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            while proc.poll() is None:
                if not (notified or main_thread.is_alive()):
                    print("Hound is still updating records in the background. Python will close when it's done")
                    notified = True
                time.sleep(10)
            assert proc.returncode == 0
    except:
        traceback.print_exc()
        warnings.warn("Fast record population failed. Switching to slower fallback")
        for i, (path, obj) in enumerate(data):
            try:
                storage.Blob(path, bucket).upload_from_string(json.dumps(obj))
            except:
                traceback.print_exc()
                print("Hound record", path, "could not be written", file=sys.stderr)
            if i % 10 == 0 and not (notified or main_thread.is_alive()):
                print("Hound is still updating", len(data) - i, "records in the background. Python will close when it's done")
                notified = True

TIMESTAMP_FORMAT = "%d/%m/%Y %H:%M:%S %Z" # always UTC

# hound/(entity)/(entity id)/(attribute) : For updates to entity attributes (update object)
# hound/(entity)/(entity id)/__meta__ : For updates to entities themselves (creation/deletion) (log object)
# hound/workspace/(attribute) : For updates to workspace attributes (update object)
# hound/workspace/__meta__ : For updates to workspace itself (creation/acl) (log object)
# hound/logs/{job,upload} : For logs of when jobs or uploads were performed (log object)
# hound/logs/other : For arbitrary user-generated log entries (log object)
# hound/logs/meta : For meta-logs, generated by hound itself (log object)

LogEntry = namedtuple("LogEntry", ['entities', 'timestamp', 'author', 'text'])
AttributeUpdate = namedtuple("AttributeUpdate", ['entityType', 'entityName', 'attributeName', 'attributeValue', 'timestamp', 'updateReason', 'author'])

def autofill_reason(func):
    """
    Decorator which allows autofilling the 'reason' argument using a contextual
    binding
    """

    signature = inspect.signature(func)

    @wraps(func)
    def call_with_reason(self, *args, **kwargs):
        arguments = signature.bind(self, *args, **kwargs).arguments
        if 'reason' not in arguments or arguments['reason'] is None:
            if self.get_current_reason() is not None:
                arguments['reason'] = self.context_reason.reason
            else:
                arguments['reason'] = "No reason given"
        return func(**arguments)

    return call_with_reason

class HoundClient(object):
    def __init__(self, bucket_id, snowflake_client='__COMMON__', credentials=None, user_project=None):
        """
        Constructs a new Hound Client
        Must provide bucket_id of firecloud workspace

        snowflake_client: The client or client name to use for generating snowflakes
        If left out, a default client will be used

        credentials: Google Cloud credentials object to use if not using application
        default credentials

        user_project: string project id to bill for usage, if using a requester-pays bucket
        """

        self.bucket = _getblob_bucket(credentials, bucket_id, user_project)
        self.snowflake_client = (
            snowflake_client if isinstance(snowflake_client, SnowflakeClient)
            else getclient(snowflake_client)
        )
        self.author = '{}@{}'.format(getuser(), gethostname())
        try:
            request = AuthorizedSession(self.bucket.client._credentials).get('https://www.googleapis.com/oauth2/v1/tokeninfo')
            if request.status_code == 200:
                self.author = request.json()['email']
        except:
            traceback.print_exc()
            print("Unable to check user credentials. Using fallback author value")
        self.context_reason = threading.local()
        self._batch = threading.local()
        self._enabled = True

    def get_current_reason(self):
        if not hasattr(self.context_reason, 'reason'):
            self.context_reason.reason = None
        return self.context_reason.reason

    @contextmanager
    def batch(self):
        """
        Prepares a background batch update
        """
        if not hasattr(self._batch, 'batch'):
            self._batch.batch = None
        if self._batch.batch is not None:
            warnings.warn("Nested batch requests append to the outermost batch", stacklevel=4)
            yield
        else:
            try:
                self._batch.batch = []
                yield
                if len(self._batch.batch):
                    threading.Thread(
                        target=_bulk_upload,
                        args=(self.bucket, self._batch.batch),
                        daemon=False,
                        name="Hound batch upload"
                    ).start()
            finally:
                self._batch.batch = None

    @contextmanager
    def with_reason(self, reason):
        """
        Provide a reason which will be used as the default 'reason' value in the context
        """
        old_reason = self.get_current_reason()
        try:
            self.context_reason.reason = reason
            yield
        finally:
            self.context_reason.reason = old_reason


    @property
    def log_volume(self):
        """
        Tuple: (number of hound entires, total size of hound entries)
        """
        size = 0

        for i, blob in enumerate(blob for page in self.bucket.list_blobs(prefix='hound', fields='items/name,items/size,nextPageToken').pages for blob in page):
            size += blob.size
        return (i+1, size)

    def write(self, path, data):
        """
        Low-level write
        Posts the given object to the path
        Object may be a JSON dictionary, LogEntry, or AttributeUpdate
        Timestamp may be omitted or None, which will be filled automatically
        Author may be omitted or None, which will be filled automatically
        """
        if isinstance(data, LogEntry):
            data = {
                'entities': data.entities,
                'timestamp': data.timestamp,
                'author': data.author,
                'text': data.text
            }
        elif isinstance(data, AttributeUpdate):
            data = {
                'entityType': data.entityType,
                'entityName': data.entityName,
                'attributeName': data.attributeName,
                'attributeValue': data.attributeValue,
                'timestamp': data.timestamp,
                'updateReason': data.updateReason,
                'author': data.author
            }
        if 'author' not in data or data['author'] is None:
            data['author'] = self.author
        if 'timestamp' not in data or data['timestamp'] is None:
            data['timestamp'] = time.strftime(TIMESTAMP_FORMAT, time.gmtime())
        path = os.path.join(path, self.snowflake_client.snowflake().hex())
        if self._enabled:
            if hasattr(self._batch, 'batch') and self._batch.batch is not None:
                self._batch.batch.append((path, data))
            else:
                storage.Blob(path, self.bucket).upload_from_string(json.dumps(data))
        return path

    def disable(self):
        """
        Disables the hound client
        """
        self._enabled = False

    def enable(self):
        """
        Enables the hound client, if disabled
        """
        self._enabled = True

    @autofill_reason
    def update_entity_attribute(self, etype, entity, attribute, value, reason=None):
        """
        Log an update to entity attributes
        etype: entity type
        entity: entity name
        attribute: attribute name
        value: value attribute
        reason: (optional) reason attribute was updated

        Also adds an entry to the meta log
        returns blob for entity log
        """
        gs_path = self.write(
            os.path.join('hound', etype, entity, attribute),
            {
                'entityType': etype,
                'entityName': entity,
                'attributeName': attribute,
                'attributeValue': value,
                'updateReason': reason
            }
        )
        self.write(
            'hound/logs/meta',
            {
                'entities': [os.path.join(etype, entity)],
                'text': 'snowflake={}; Updated attribute: {}'.format(
                    os.path.basename(gs_path),
                    attribute
                )
            }
        )
        return gs_path

    @autofill_reason
    def update_entity_meta(self, etype, entity, event, reason=None):
        """
        Log a creation, deletion, or other meta-event on an entity
        etype: entity type
        entity: entity id
        event: what occurred
        reason: (optional) reason the event occurred

        Also adds an entry to the meta log
        returns blob for entity log
        """
        gs_path = self.write(
            os.path.join('hound', etype, entity, '__meta__'),
            {
                'entityType': etype,
                'entityName': entity,
                'attributeName': '__meta__',
                'attributeValue': event,
                'updateReason': reason
            }
        )
        self.write(
            'hound/logs/meta',
            {
                'entities': [os.path.join(etype, entity)],
                'text': 'snowflake={}; Modified {} (meta-event)'.format(
                    os.path.basename(gs_path),
                    etype
                )
            }
        )
        return gs_path

    @autofill_reason
    def update_workspace_attribute(self, attribute, value, reason=None):
        """
        Log an update to workspace attributes
        attribute: attribute name
        value: value attribute
        reason: (optional) reason attribute was updated

        Also adds an entry to the meta log
        returns blob for entity log
        """
        gs_path = self.write(
            os.path.join('hound', 'workspace', attribute),
            {
                'entityType': 'workspace',
                'entityName': 'workspace',
                'attributeName': attribute,
                'attributeValue': value,
                'updateReason': reason
            }
        )
        self.write(
            'hound/logs/meta',
            {
                'entities': ['workspace'],
                'text': 'snowflake={}; Updated attribute: {}'.format(
                    os.path.basename(gs_path),
                    attribute
                )
            }
        )
        return gs_path

    @autofill_reason
    def update_workspace_meta(self, event, reason=None):
        """
        Log a creation, deletion, or other meta-event on the workspace
        event: what occurred
        reason: (optional) reason the event occurred

        Also adds an entry to the meta log
        returns blob for entity log
        """
        gs_path = self.write(
            'hound/workspace/__meta__',
            {
                'entityType': 'workspace',
                'entityName': 'workspace',
                'attributeName': '__meta__',
                'attributeValue': event,
                'updateReason': reason
            }
        )
        self.write(
            'hound/logs/meta',
            {
                'entities': ['workspace'],
                'text': 'snowflake={}; Modified Workspace (meta-event)'.format(
                    os.path.basename(gs_path),
                )
            }
        )
        return gs_path

    def write_log_entry(self, log_type, text, entities=None):
        """
        Write a new entry to the log
        log_type must be one of {'job', 'upload', 'other'}

        text: Log text
        entities: (optional) list of entities relevant to this log entry
        For consistency, entity names should be in format "entity_type/entity_name"

        Also adds an entry to the meta log
        returns blob for entity log
        """
        if log_type not in {'job', 'upload', 'other'}:
            raise TypeError("log_type must be in {'job', 'upload', 'other'}")
        gs_path = self.write(
            os.path.join('hound', 'logs', log_type),
            {
                'entities': entities,
                'text': text
            }
        )
        self.write(
            'hound/logs/meta',
            {
                'entities': [os.path.join('logs', log_type)],
                'text': 'snowflake={}; Added entry to "{}" log'.format(
                    os.path.basename(gs_path),
                    log_type
                )
            }
        )
        return gs_path

    def get_entries(self, path):
        """
        Iterates over log entires at a given path
        """
        if not path.endswith('/'):
            path += '/'
        for page in self.bucket.list_blobs(prefix=path, fields='items/name,nextPageToken').pages:
            yield from page

    def _yield_logs(self, src):
        for entry in src:
            data = json.loads(entry.download_as_string())
            yield LogEntry(data['entities'], data['timestamp'], data['author'], data['text'])

    def _yield_updates(self, src):
        for entry in src:
            data = json.loads(entry.download_as_string())
            yield AttributeUpdate(
                data['entityType'],
                data['entityName'],
                data['attributeName'],
                data['attributeValue'],
                data['timestamp'],
                data['updateReason'],
                data['author']
            )

    def latest(self, src, entryType=None):
        """
        Returns the most recent object from a source iterator
        src: google.cloud.storage.bucket_iterator (as returned by HoundClient.get_entries) OR string hound path (ie: "hound/workspace/foo")
        entryType: (optional) Return the latest object as a parsed object type instead of a blob
        {entryType may be "logs" or "updates"}

        Returns the latest object from the given iterator or None
        """
        if isinstance(src, list):
            src = iter(src)
        elif isinstance(src, str):
            src = self.get_entries(src)
        if entryType is not None:
            if entryType == 'logs':
                entryType = lambda x: [*self._yield_logs((x,))][0]
            elif entryType == 'updates':
                entryType = lambda x: [*self._yield_updates((x,))][0]
            else:
                raise TypeError("entryType must be in {'logs', 'updates'}")
        latest = None
        latest_blob = None
        for entry in src:
            snowflake = self.snowflake_client.unpack(os.path.basename(entry.name))
            if latest is None or snowflake.time > latest.time:
                latest = snowflake
                latest_blob = entry
        return latest_blob if latest_blob is None or entryType is None else entryType(latest_blob)

    def iread_log(self, log_type, since=None):
        """
        Iterates over log entries from the requested log type
        log_type must be one of: 'job', 'upload', 'other', 'meta'
        Since should be a unit timestamp or python struct_time object (UTC)
        Only logs since that value will be returned
        Probably ordered, but this is not guaranteed by Google
        """
        yield from self._yield_logs(self._iread_log_internal(log_type, since))

    def _iread_log_internal(self, log_type, since):
        """
        Backend for iread_log
        Yields blob objects, since they are faster to list
        """
        if log_type not in {'job', 'upload', 'other', 'meta'}:
            raise TypeError("log_type must be one of {'job', 'upload', 'other', 'meta'}")
        if isinstance(since, time.struct_time):
            since = calendar.timegm(since)
        for entry in self.get_entries('hound/logs/{}/'.format(log_type)):
            if since is None or since <= self.snowflake_client.unpack(os.path.basename(entry.name)).time:
                yield entry

    def read_log(self, log_type, since=None):
        """
        Returns a list of entries of the requested log type
        log_type must be one of: 'job', 'upload', 'other', 'meta'
        Since should be a unit timestamp or python struct_time object (UTC)
        Only logs since that value will be returned
        Sorted by timestamp from oldest to most recent
        """
        return sorted(
            self.iread_log(log_type, since),
            key=lambda entry: time.strptime(entry.timestamp, TIMESTAMP_FORMAT)
        )

    def ientity_attribute_provenance(self, etype, entity=None, attribute=None):
        """
        Iterates over attribute updates for the requested enity type
        etype: Entity type
        entity: (optional) entity name
        attribute: (optional) attribute

        If entity is not given, iterate over all entities
        If attribute is not given, iterate over all attributes
        Probably ordered, but this is not guaranteed by Google
        """
        if entity is not None and attribute is None:
            source = self.get_entries(os.path.join('hound', etype, entity))
        elif entity is None or attribute is None:
            source = self.get_entries(os.path.join('hound', etype))
        elif entity is not None and attribute is not None:
            source = self.get_entries(os.path.join('hound', etype, entity, attribute))
        for entry in self._yield_updates(source):
            if (attribute is None and entry.attributeName != '__meta__') or entry.attributeName == attribute:
                yield entry


    def iworkspace_attribute_provenance(self, attribute=None):
        """
        Iterates over attribute updates for the workspace
        attribute: (optional) attribute name

        If attribute is not given, iterate over all attributes
        Probably ordered, but this is not guaranteed by Google
        """
        if attribute is None:
            source = self.get_entries('hound/workspace')
        else:
            source = self.get_entries(os.path.join('hound', 'workspace', attribute))
        for entry in self._yield_updates(source):
            if (attribute is None and entry.attributeName != '__meta__') or entry.attributeName == attribute:
                yield entry

    def entity_attribute_provenance(self, etype, entity=None, attribute=None):
        """
        Returns a list of attribute updates for the requested enity type
        etype: Entity type
        entity: (optional) entity name
        attribute: (optional) attribute

        If entity is not given, iterate over all entities
        If attribute is not given, iterate over all attributes
        Sorted from oldest to most recent
        """
        return sorted(
            self.ientity_attribute_provenance(etype, entity, attribute),
            key=lambda entry: time.strptime(entry.timestamp, TIMESTAMP_FORMAT)
        )

    def workspace_attribute_provenance(self, attribute=None):
        """
        Returns a list of attribute updates for the workspace
        attribute: (optional) attribute name

        If attribute is not given, iterate over all attributes
        Probably ordered, but this is not guaranteed by Google
        """
        return sorted(
            self.iworkspace_attribute_provenance(attribute),
            key=lambda entry: time.strptime(entry.timestamp, TIMESTAMP_FORMAT)
        )

    @autofill_reason
    def record_entity_upload(self, etype, data, entity_id=None, reason=None):
        """
        Record an entity upload
        etype: entity type
        data: dict or pandas series containing entity metadata
        entity_id: (optional) id if data is a dictionary or the series index is not the id
        reason: (optional) Reason why the entity is being uploaded

        Actions:
        1) Writes to upload log
        2) Writes to entity meta log (entity creation)
        3) Writes to entity attribute log (for each attribute)
        """
        if entity_id is None:
            entity_id = data.name
        self.write_log_entry('upload', 'Uploading new entity', [os.path.join(etype, entity_id)])
        self.update_entity_meta(etype, entity_id, "User uploaded new entity", reason)
        for attribute, value in data.items():
            self.update_entity_attribute(etype, entity_id, attribute, value, reason)

    def get_record_from_snowflake(self, snowflake):
        """
        Fetches an entry in the Hound database from it's snowflake (filename).
        Useful for finding a log entry using the snowflake given in a meta log entry
        """
        for page in self.bucket.list_blobs(prefix='hound', fields='items/name,nextPageToken').pages:
            for blob in page:
                if os.path.basename(blob.name) == snowflake:
                    return json.loads(blob.download_as_string())
