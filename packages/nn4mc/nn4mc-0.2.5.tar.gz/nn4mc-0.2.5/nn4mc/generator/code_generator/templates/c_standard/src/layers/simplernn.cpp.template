<%BEGIN_DEFINITION_TEMPLATE>
/********************
    simplernn.cpp

    Code generated using nn4mc.

    This file implements a simple recurret layer.

    Implementation of Elman Network:
        Elman, Jeffrey L. (1990). Finding Structure in Time. Cognitive Science.
            14 (2): 179â€“211.

*/

#include "simplernn.h"
#include "activations.h"
#include <math.h>
#include <stdlib.h>

#define max(a, b) (((a)>(b) ? (a) : (b)))
#define min(a, b) (((a)<(b) ? (a) : (b)))

struct SimpleRNN buildSimpleRNN(<%WEIGHT_DATATYPE_DELIMITER> * W, <%WEIGHT_DATATYPE_DELIMITER> * Wrec, <%WEIGHT_DATATYPE_DELIMITER> * b, <%INDEX_DATATYPE_DELIMITER> input_sh0, <%INDEX_DATATYPE_DELIMITER> input_sh1, <%INDEX_DATATYPE_DELIMITER> output_sh, <%ACTIVATION_DATATYPE_DELIMITER> activation, bool go_backwards)
{
	struct SimpleRNN layer;

	layer.weights = W;
	layer.biases = b;

    layer.w_rec = Wrec;
    layer.input_shape[0] = input_sh0;
    layer.input_shape[1] = input_sh1;

    layer.activation = activation;

    layer.window_size= output_sh;
    layer.output_shape = output_sh;
    layer.go_backwards = go_backwards;

    layer.prev_h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_shape * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    for (int i=0; i<L.output_shape; i++) L.prev_h[i] = 0.0;

	return layer;
}

void reverseInput(<%LAYER_DATATYPE_DELIMITER> * x, int start, int end){
    while(start < end){
        <%LAYER_DATATYPE_DELIMITER> temp = x[start];
        x[start] = x[end];
        x[end] = temp;
        start++;
        end--;
    }
}

<%LAYER_DATATYPE_DELIMITER> * fwdSimpleRNN(struct SimpleRNN L, <%LAYER_DATATYPE_DELIMITER>* input)
{
    if (layer.go_backwards){
        reverseInput(x, 0, L.input_shape[0]*L.input_shape[1] - 1);
    }

    <%LAYER_DATATYPE_DELIMITER> * h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_shape * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    <%LAYER_DATATYPE_DELIMITER> * y = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_shape * sizeof(<%LAYER_DATATYPE_DELIMITER>));

        for (int j = 0 ; j < L.output_shape; j++){

        h[j] = L.biases[j];

        for (int i = 0; i < L.input_shape[0]*L.input_shape[1]; i++){
            h[j] += *(L.weights + i*L.output_shape + j) * input[i];
        }

        for (int i = 0; i < L.output_shape; i++){
            h[j] += *(L.w_rec + i * L.output_shape + j) * L.prev_h[j];
        }

        if (L.activation != 0xB)
            h[j] = activate(h[j], L.output_shape, L.activation);
        }

        for (int i = 0 ; i < L.output_shape ; i++){ // Actualizing previous hidden
            L.prev_h[i] =  h[i];
        }

        for (int j = 0; j < L.output_shape; j++){
            y[j] = L.biases[j];

            for (int i = 0; i < L.output_shape; i++){
                y[j] += *(L.weights + i*L.output_shape + j) * h[j];
            }

            if (L.activation != 0xB)
                y[j] = activate(y[j], L.output_shape, L.activation);
        }

    free(h);
    return y;
}

<%END_DEFINITION_TEMPLATE>
