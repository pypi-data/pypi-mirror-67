<%BEGIN_DEFINITION_TEMPLATE>
/********************
    gru.cpp

    Code generated using nn4mc.

    This file implements a gated recurrent unit layer.

*/

#include "gru.h"
#include "activation_func.h"
#include <math.h>
#include <stdlib.h>

#define max(a, b) (((a)>(b) ? (a) : (b)))
#define min(a, b) (((a)<(b) ? (a) : (b)))

struct GRU buildGRU(<%WEIGHT_DATATYPE_DELIMITER> * W, <%WEIGHT_DATATYPE_DELIMITER> * Wrec, <%WEIGHT_DATATYPE_DELIMITER> * b, <%INDEX_DATATYPE_DELIMITER> input_sh0, <%INDEX_DATATYPE_DELIMITER> input_sh1, <%INDEX_DATATYPE_DELIMITER> output_sh, <%ACTIVATION_DATATYPE_DELIMITER> activation, <%ACTIVATION_DATATYPE_DELIMITER> recurrent_activation, <%LAYER_DATATYPE_DELIMITER> dropout,   <%LAYER_DATATYPE_DELIMITER>
recurrent_dropout, bool go_backwards)
{
	struct GRU layer;

	layer.weights = W;
	layer.biases = b;
    layer.wrec = Wrec

    layer.input_shape[0] = input_sh0;
    layer.input_shape[1] = input_sh1;

    layer.activation = activation;
    layer.recurrent_activation = recurrent_activation;

    layer.output_size = output_sh;

    layer.dropout = dropout;
    layer.recurrent_dropout = recurrent_dropout;

    layer.go_backwards = go_backwards;

    layer.prev_h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size *sizeof(<%LAYER_DATATYPE_DELIMITER>));
    for (int i = 0; i< layer.output_size; i++) layer.prev_h[i] = 0.0;

    return layer;
}

void reverseInput(<%LAYER_DATATYPE_DELIMITER> * x, int start, int end){
    while(start < end){
        <%LAYER_DATATYPE_DELIMITER> temp = x[start];
        x[start] = x[end];
        x[end] = temp;
        start++;
        end--;
    }
}

<%LAYER_DATATYPE_DELIMITER> * fwdGRU(struct GRU L, <%LAYER_DATATYPE_DELIMITER>* input)
{

    if (layer.go_backwards){
        reverseInput(input, 0, L.input_shape[0]*L.input_shape[1] - 1);
    }

    <%LAYER_DATATYPE_DELIMITER> * h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size*sizeof(<%LAYER_DATATYPE_DELIMITER>));

     <%LAYER_DATATYPE_DELIMITER> * z = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));
     <%LAYER_DATATYPE_DELIMITER> * r = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    <%LAYER_DATATYPE_DELIMITER> * hh = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));

    for (int i=0; i< L.output_size; i++){
        <%INDEX_DATATYPE_DELIMITER> int_z = i;
        <%INDEX_DATATYPE_DELIMITER> int_r = 1*L.output_size + i;
        <%INDEX_DATATYPE_DELIMTIER> int_h = 2*L.output_size + i;

        z[i] = L.biases[int_z];
        r[i] = L.biases[int_r];
        hh[i] = L.biases[int_h];

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j< L.input_shape[0]*L.input_shape[1]; j++){
            z[i]+= *(L.weights + ind_z*L.output_size + j)*input[j];
            r[i]+= *(L.weights + ind_r*L.output_size + j)*input[j];
            hh[i]+= *(L.weights + ind_h*L.output_size + j)*input[j];
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            z[i]+= *(L.wrec + ind_z*L.output_size + j) * L.prev_h[j];
            r[i]+= *(L.wrec + ind_r*L.output_size + j) * L.prev_h[j];
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0;j<L.output_size;j++){
            z[i] = activate(z[i], L.output_size, L.recurrent_activation);
            r[i] = activate(r[i], L.output_size, L.recurrent_activation);
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            hh[i]+= *(L.wrec + ind_h*L.output_shape + j) *(r[j]*L.prev_h[j]);
       }

      for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            hh[i]= activate(hh[i], L.output_shape, L.activation);
            h[i] = z[i]*L.prev_h[i] + (1 - z[i]) * hh[i];
            L.prev_h[i] = h[i];
       }

    }

    free(z);
    free(input);
    free(r);
    free(hh);

    return h;
}

<%END_DEFINITION_TEMPLATE>
