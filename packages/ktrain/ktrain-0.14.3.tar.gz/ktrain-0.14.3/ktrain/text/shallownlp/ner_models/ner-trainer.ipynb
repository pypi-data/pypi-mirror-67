{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"; \n",
    "os.environ['DISABLE_V2_BEHAVIOR'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DISABLE_V2_BEHAVIOR with TensorFlow\n"
     ]
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data sample:\n",
      "   SentenceID            Word        Tag\n",
      "0           0            IL-2  B-protein\n",
      "1           0  responsiveness          O\n",
      "2           0        requires          O\n",
      "3           0           three          O\n",
      "4           0        distinct          O\n",
      "Number of sentences:  2\n",
      "Number of words in the dataset:  10\n",
      "Tags: ['B-protein', 'B-DNA', 'O']\n",
      "Number of Labels:  3\n",
      "Longest sentence: 10 words\n",
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tBERT embeddings with monologg/biobert_v1.1_pubmed\n",
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.6563 - val_loss: 1.9778\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 2.0310 - val_loss: 0.7405\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.6970 - val_loss: 0.5403\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.6017 - val_loss: 0.4072\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.4419 - val_loss: 0.3782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92328c5b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train= [['IL-2', 'responsiveness', 'requires', 'three', 'distinct', 'elements', 'within', 'the', 'enhancer', '.'],\n",
    "         ['IL-2', 'responsiveness', 'requires', 'three', 'distinct', 'elements', 'within', 'the', 'enhancer', '.']]\n",
    "y_train=[['B-protein', 'O', 'O', 'O', 'O', 'B-DNA', 'O', 'O', 'B-DNA', 'O'],\n",
    "        ['B-protein', 'O', 'O', 'O', 'O', 'B-DNA', 'O', 'O', 'B-DNA', 'O']]\n",
    "(trn, val, preproc) = txt.entities_from_array(x_train, y_train)\n",
    "model = txt.sequence_tagger('bilstm-bert', preproc, bert_model='monologg/biobert_v1.1_pubmed')\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 1, cycle_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IL',\n",
       "  '-',\n",
       "  '2',\n",
       "  'responsiveness',\n",
       "  'requires',\n",
       "  'three',\n",
       "  'distinct',\n",
       "  'elements',\n",
       "  'within',\n",
       "  'the',\n",
       "  'enhancer',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  14041\n",
      "Number of words in the dataset:  23623\n",
      "Tags: ['B-PER', 'B-MISC', 'I-ORG', 'I-MISC', 'B-ORG', 'I-PER', 'I-LOC', 'B-LOC', 'O']\n",
      "Number of Labels:  9\n",
      "Longest sentence: 113 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaiya/projects/ghub/ktrain/ktrain/text/ner/models.py:123: UserWarning: Setting use_char=False:  character embeddings cannot be used in TF2 due to open TensorFlow 2 bug (#33148).\n",
      "Add os.environ[\"DISABLE_V2_BEHAVIOR\"] = \"1\" to the top of script if you really want to use it.\n",
      "  'Add os.environ[\"DISABLE_V2_BEHAVIOR\"] = \"1\" to the top of script if you really want to use it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tBERT embeddings with monologg/biobert_v1.1_pubmed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 09:32:35.435680 140272807765824 filelock.py:274] Lock 140265857300688 acquired on /home/amaiya/.cache/torch/transformers/a7d51998f3e1033c9f4d16d0c96f2de1c78e1e234ec025a216e45e7b81e72a8e.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf9f1562bc34074b6401d674393ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=112, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 09:32:35.564820 140272807765824 filelock.py:318] Lock 140265857300688 released on /home/amaiya/.cache/torch/transformers/a7d51998f3e1033c9f4d16d0c96f2de1c78e1e234ec025a216e45e7b81e72a8e.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n",
      "I0330 09:32:35.617287 140272807765824 filelock.py:274] Lock 140265858737824 acquired on /home/amaiya/.cache/torch/transformers/411e4b56adae7178368f3bdd9a9040dbc43685308ce8d88fae3e0c21dfcb9255.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f2219e05504425bdd29c7028d12ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=24, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 09:32:35.690016 140272807765824 filelock.py:318] Lock 140265858737824 released on /home/amaiya/.cache/torch/transformers/411e4b56adae7178368f3bdd9a9040dbc43685308ce8d88fae3e0c21dfcb9255.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "Train for 110 steps, validate for 13 steps\n",
      "Epoch 1/5\n",
      "110/110 [==============================] - 61s 552ms/step - loss: 0.1532 - val_loss: 0.0462\n",
      "Epoch 2/5\n",
      "110/110 [==============================] - 53s 486ms/step - loss: 0.0422 - val_loss: 0.0280\n",
      "Epoch 3/5\n",
      "110/110 [==============================] - 54s 488ms/step - loss: 0.0231 - val_loss: 0.0242\n",
      "Epoch 4/5\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 0.0164 - val_loss: 0.0229\n",
      "Epoch 5/5\n",
      "110/110 [==============================] - 53s 482ms/step - loss: 0.0138 - val_loss: 0.0223\n",
      "   F1: 84.28\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     MISC       0.70      0.77      0.73       922\n",
      "      LOC       0.88      0.90      0.89      1837\n",
      "      PER       0.91      0.91      0.91      1842\n",
      "      ORG       0.74      0.79      0.76      1341\n",
      "\n",
      "micro avg       0.83      0.86      0.84      5942\n",
      "macro avg       0.83      0.86      0.84      5942\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8427839312283022"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDATA = 'data/conll2003/train.txt'\n",
    "VDATA = 'data/conll2003/valid.txt'\n",
    "(trn, val, preproc) = txt.entities_from_conll2003(TDATA, val_filepath=VDATA, use_char=True)\n",
    "model = txt.sequence_tagger('bilstm-bert', preproc, bert_model='monologg/biobert_v1.1_pubmed')\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 1, cycle_len=5)\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  14041\n",
      "Number of words in the dataset:  23623\n",
      "Tags: ['O', 'I-MISC', 'B-ORG', 'B-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-PER', 'B-LOC']\n",
      "Number of Labels:  9\n",
      "Longest sentence: 113 words\n",
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tcharacter embeddings\n",
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "Epoch 1/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.6142Epoch 1/128\n",
      "110/110 [==============================] - 44s 402ms/step - loss: 13.6169 - val_loss: 17.7142\n",
      "Epoch 2/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2841Epoch 1/128\n",
      "110/110 [==============================] - 35s 321ms/step - loss: 13.2452 - val_loss: 17.6824\n",
      "Epoch 3/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2666Epoch 1/128\n",
      "110/110 [==============================] - 35s 323ms/step - loss: 13.2104 - val_loss: 17.6727\n",
      "Epoch 4/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.1985Epoch 1/128\n",
      "110/110 [==============================] - 35s 322ms/step - loss: 13.1983 - val_loss: 17.6788\n",
      "Epoch 5/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.1876Epoch 1/128\n",
      "110/110 [==============================] - 36s 325ms/step - loss: 13.1928 - val_loss: 17.6820\n",
      "Epoch 6/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2882Epoch 1/128\n",
      "110/110 [==============================] - 36s 325ms/step - loss: 13.1906 - val_loss: 17.6793\n",
      "Epoch 7/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2365Epoch 1/128\n",
      "110/110 [==============================] - 36s 326ms/step - loss: 13.1891 - val_loss: 17.6850\n",
      "Epoch 8/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2773Epoch 1/128\n",
      "110/110 [==============================] - 35s 323ms/step - loss: 13.1880 - val_loss: 17.6903\n",
      "Epoch 9/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.1897Epoch 1/128\n",
      "110/110 [==============================] - 35s 322ms/step - loss: 13.1868 - val_loss: 17.7108\n",
      "Epoch 10/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.1855Epoch 1/128\n",
      "110/110 [==============================] - 36s 328ms/step - loss: 13.1864 - val_loss: 17.6994\n",
      "Epoch 11/128\n",
      "109/110 [============================>.] - ETA: 0s - loss: 13.2540Epoch 1/128\n",
      "110/110 [==============================] - 37s 332ms/step - loss: 13.1875 - val_loss: 17.7006\n",
      "Weights from best epoch have been loaded into model.\n",
      "   F1: 87.73\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     MISC       0.89      0.77      0.83       922\n",
      "      ORG       0.75      0.86      0.80      1341\n",
      "      LOC       0.93      0.91      0.92      1837\n",
      "      PER       0.94      0.90      0.92      1842\n",
      "\n",
      "micro avg       0.88      0.87      0.88      5942\n",
      "macro avg       0.89      0.87      0.88      5942\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773441459706031"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDATA = 'data/conll2003/train.txt'\n",
    "VDATA = 'data/conll2003/valid.txt'\n",
    "(trn, val, preproc) = txt.entities_from_conll2003(TDATA, val_filepath=VDATA, use_char=True)\n",
    "model = txt.sequence_tagger('bilstm-crf', preproc)\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 1, cycle_len=128, early_stopping=8)\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Xuetao', 'B-PER'),\n",
       " ('Cao', 'I-PER'),\n",
       " ('was', 'O'),\n",
       " ('head', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Chinese', 'B-ORG'),\n",
       " ('Academy', 'I-ORG'),\n",
       " ('of', 'I-ORG'),\n",
       " ('Medical', 'I-ORG'),\n",
       " ('Sciences', 'I-ORG'),\n",
       " ('and', 'O'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('current', 'O'),\n",
       " ('president', 'O'),\n",
       " ('of', 'O'),\n",
       " ('Nankai', 'B-ORG'),\n",
       " ('University', 'I-ORG'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)\n",
    "text = \"\"\"\n",
    "Xuetao Cao was head of the Chinese Academy of Medical Sciences and is \n",
    "the current president of Nankai University.\n",
    "\"\"\"\n",
    "p.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)\n",
    "p.save('/tmp/ner_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F1: 87.73\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     MISC       0.89      0.77      0.83       922\n",
      "      ORG       0.75      0.86      0.80      1341\n",
      "      LOC       0.93      0.91      0.92      1837\n",
      "      PER       0.94      0.90      0.92      1842\n",
      "\n",
      "micro avg       0.88      0.87      0.88      5942\n",
      "macro avg       0.89      0.87      0.88      5942\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773441459706031"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ktrain' has no attribute 'load_predictorictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1683a926939e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_predictorictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/ner_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ktrain' has no attribute 'load_predictorictor'"
     ]
    }
   ],
   "source": [
    "p = ktrain.load_predictorictor('/tmp/ner_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paul', 'B-PER'),\n",
       " ('Newman', 'I-PER'),\n",
       " ('is', 'O'),\n",
       " ('my', 'O'),\n",
       " ('favorite', 'O'),\n",
       " ('American', 'B-MISC'),\n",
       " ('actor', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict('Paul Newman is my favorite American actor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  20864\n",
      "Number of words in the dataset:  4312\n",
      "Tags: ['O', 'B-PER', 'B-ORG', 'I-ORG', 'I-LOC', 'I-PER', 'B-LOC']\n",
      "Number of Labels:  7\n",
      "Longest sentence: 574 words\n",
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tcharacter embeddings\n",
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "Epoch 1/5\n",
      "162/163 [============================>.] - ETA: 1s - loss: 10.4087Epoch 1/5\n",
      "163/163 [==============================] - 186s 1s/step - loss: 10.3837 - val_loss: 13.5322\n",
      "Epoch 2/5\n",
      "162/163 [============================>.] - ETA: 1s - loss: 10.2417Epoch 1/5\n",
      "163/163 [==============================] - 177s 1s/step - loss: 10.2524 - val_loss: 13.5059\n",
      "Epoch 3/5\n",
      "162/163 [============================>.] - ETA: 1s - loss: 10.2605Epoch 1/5\n",
      "163/163 [==============================] - 177s 1s/step - loss: 10.2305 - val_loss: 13.4968\n",
      "Epoch 4/5\n",
      "162/163 [============================>.] - ETA: 1s - loss: 10.2444Epoch 1/5\n",
      "163/163 [==============================] - 179s 1s/step - loss: 10.2202 - val_loss: 13.4934\n",
      "Epoch 5/5\n",
      "162/163 [============================>.] - ETA: 1s - loss: 10.2182Epoch 1/5\n",
      "163/163 [==============================] - 178s 1s/step - loss: 10.2160 - val_loss: 13.4932\n",
      "   F1: 83.64\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.85      0.84      0.85      3658\n",
      "      PER       0.90      0.87      0.88      1864\n",
      "      ORG       0.80      0.76      0.78      2185\n",
      "\n",
      "micro avg       0.85      0.82      0.84      7707\n",
      "macro avg       0.85      0.82      0.84      7707\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8364116094986807"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDATA = 'data/chinese_ner/example.train'\n",
    "VDATA = 'data/chinese_ner/example.test'\n",
    "(trn, val, preproc) = txt.entities_from_conll2003(TDATA, val_filepath=VDATA, use_char=True)\n",
    "model = txt.sequence_tagger('bilstm-crf', preproc)\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 1, cycle_len=5)\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)\n",
    "p.save('/tmp/ner_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('曹', 'B-PER'),\n",
       " ('雪', 'I-PER'),\n",
       " ('涛', 'I-PER'),\n",
       " ('曾', 'O'),\n",
       " ('任', 'O'),\n",
       " ('中', 'B-ORG'),\n",
       " ('国', 'I-ORG'),\n",
       " ('医', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('科', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('院', 'I-ORG'),\n",
       " ('院', 'O'),\n",
       " ('长', 'O'),\n",
       " ('，', 'O'),\n",
       " ('现', 'O'),\n",
       " ('任', 'O'),\n",
       " ('南', 'B-ORG'),\n",
       " ('开', 'I-ORG'),\n",
       " ('大', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('校', 'O'),\n",
       " ('长', 'O'),\n",
       " ('。', 'O')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict('曹雪涛曾任中国医学科学院院长，现任南开大学校长。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('曹', 'B-PER'),\n",
       " ('雪', 'I-PER'),\n",
       " ('涛', 'I-PER'),\n",
       " ('曾', 'O'),\n",
       " ('任', 'O'),\n",
       " ('中', 'B-ORG'),\n",
       " ('国', 'I-ORG'),\n",
       " ('医', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('科', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('院', 'I-ORG'),\n",
       " ('院', 'O'),\n",
       " ('长', 'O'),\n",
       " ('，', 'O'),\n",
       " ('现', 'O'),\n",
       " ('任', 'O'),\n",
       " ('南', 'B-ORG'),\n",
       " ('开', 'I-ORG'),\n",
       " ('大', 'I-ORG'),\n",
       " ('学', 'I-ORG'),\n",
       " ('校', 'O'),\n",
       " ('长', 'O'),\n",
       " ('。', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict('曹雪涛曾任中国医学科学院院长，现任南开大学校长。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  12599\n",
      "Number of words in the dataset:  35687\n",
      "Tags: ['O', 'B-ORG', 'B-PER', 'I-ORG', 'I-LOC', 'I-PER', 'B-LOC']\n",
      "Number of Labels:  7\n",
      "Longest sentence: 222 words\n",
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tcharacter embeddings\n",
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "Epoch 1/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.4224Epoch 1/10\n",
      "89/89 [==============================] - 51s 578ms/step - loss: 8.3805 - val_loss: 7.6363\n",
      "Epoch 2/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.1966Epoch 1/10\n",
      "89/89 [==============================] - 41s 466ms/step - loss: 8.1540 - val_loss: 7.6166\n",
      "Epoch 3/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.1228Epoch 1/10\n",
      "89/89 [==============================] - 41s 460ms/step - loss: 8.1104 - val_loss: 7.6238\n",
      "Epoch 4/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.0989Epoch 1/10\n",
      "89/89 [==============================] - 41s 464ms/step - loss: 8.0840 - val_loss: 7.6344\n",
      "Epoch 5/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 7.9556Epoch 1/10\n",
      "89/89 [==============================] - 41s 465ms/step - loss: 8.0719 - val_loss: 7.6386\n",
      "Epoch 6/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.1075Epoch 1/10\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 8.0902 - val_loss: 7.6404\n",
      "Epoch 7/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.0869Epoch 1/10\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 8.0776 - val_loss: 7.6371\n",
      "Epoch 8/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.0766Epoch 1/10\n",
      "89/89 [==============================] - 41s 464ms/step - loss: 8.0603 - val_loss: 7.6633\n",
      "Epoch 9/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.0853Epoch 1/10\n",
      "89/89 [==============================] - 41s 462ms/step - loss: 8.0510 - val_loss: 7.6755\n",
      "Epoch 10/10\n",
      "88/89 [============================>.] - ETA: 0s - loss: 8.0295Epoch 1/10\n",
      "89/89 [==============================] - 41s 462ms/step - loss: 8.0476 - val_loss: 7.6775\n",
      "   F1: 85.68\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       0.88      0.89      0.88      1106\n",
      "      ORG       0.80      0.79      0.79       845\n",
      "      LOC       0.90      0.87      0.89       791\n",
      "\n",
      "micro avg       0.86      0.85      0.86      2742\n",
      "macro avg       0.86      0.85      0.86      2742\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8567760865578581"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDATA = 'data/russian_ner/russtext.csv'\n",
    "(trn, val, preproc) = txt.entities_from_gmb(TDATA, use_char=True)\n",
    "model = txt.sequence_tagger('bilstm-crf', preproc)\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 2, cycle_len=5)\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)\n",
    "p.save('/tmp/ner_russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Катерина', 'B-PER'),\n",
       " ('Тихонова', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('младшая', 'O'),\n",
       " ('дочь', 'O'),\n",
       " ('президента', 'O'),\n",
       " ('России', 'B-LOC'),\n",
       " ('Владимира', 'B-PER'),\n",
       " ('Путина', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('была', 'O'),\n",
       " ('назначена', 'O'),\n",
       " ('руководителем', 'O'),\n",
       " ('нового', 'O'),\n",
       " ('института', 'O'),\n",
       " ('искусственного', 'O'),\n",
       " ('интеллекта', 'O'),\n",
       " ('в', 'O'),\n",
       " ('МГУ', 'B-ORG'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_sentence = \"\"\"Катерина Тихонова, младшая дочь президента России Владимира Путина, \n",
    "была назначена руководителем нового института искусственного интеллекта в МГУ.\"\"\"\n",
    "p.predict(russian_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Катерина', 'B-PER'),\n",
       " ('Тихонова', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('младшая', 'O'),\n",
       " ('дочь', 'O'),\n",
       " ('президента', 'O'),\n",
       " ('России', 'B-LOC'),\n",
       " ('Владимира', 'B-PER'),\n",
       " ('Путина', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('была', 'O'),\n",
       " ('назначена', 'O'),\n",
       " ('руководителем', 'O'),\n",
       " ('нового', 'O'),\n",
       " ('института', 'O'),\n",
       " ('искусственного', 'O'),\n",
       " ('интеллекта', 'O'),\n",
       " ('в', 'O'),\n",
       " ('МГУ', 'B-ORG'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_sentence = \"\"\"Катерина Тихонова, младшая дочь президента России Владимира Путина, \n",
    "была назначена руководителем нового института искусственного интеллекта в МГУ.\"\"\"\n",
    "p.predict(russian_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  14041\n",
      "Number of words in the dataset:  23623\n",
      "Tags: ['B-MISC', 'I-PER', 'B-ORG', 'I-MISC', 'O', 'I-LOC', 'I-ORG', 'B-LOC', 'B-PER']\n",
      "Number of Labels:  9\n",
      "Longest sentence: 113 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 08:18:45.619592 140062170642240 elmo.py:102] Initializing ELMo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized randomly\n",
      "\tElmo embeddings for English\n",
      "\tcharacter embeddings\n",
      "\n",
      "preparing train data ...done.\n",
      "preparing valid data ...done.\n",
      "110/110 [==============================] - 973s 9s/step - loss: 13.4118 - val_loss: 17.6894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f61270ef588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDATA = 'data/conll2003/train.txt'\n",
    "VDATA = 'data/conll2003/valid.txt'\n",
    "(trn, val, preproc) = txt.entities_from_conll2003(TDATA, val_filepath=VDATA, use_char=True)\n",
    "model = txt.sequence_tagger('bilstm-crf-elmo', preproc)\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128, eval_batch_size=256)\n",
    "learner.fit(0.01, 1, cycle_len=1)\n",
    "#learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 08:35:15.670038 140062170642240 elmo.py:102] Initializing ELMo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Paul', 'B-PER'),\n",
       " ('Newman', 'I-PER'),\n",
       " ('is', 'O'),\n",
       " ('my', 'O'),\n",
       " ('favorite', 'O'),\n",
       " ('American', 'B-MISC'),\n",
       " ('actor', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)\n",
    "p.save('/tmp/ner_elmo')\n",
    "p = ktrain.load_predictor('/tmp/ner_elmo')\n",
    "p.predict('Paul Newman is my favorite American actor.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F1: 86.95\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.92      0.89      0.90      1837\n",
      "      PER       0.96      0.95      0.96      1842\n",
      "      ORG       0.75      0.85      0.80      1341\n",
      "     MISC       0.73      0.77      0.75       922\n",
      "\n",
      "micro avg       0.86      0.88      0.87      5942\n",
      "macro avg       0.86      0.88      0.87      5942\n",
      "\n",
      "CPU times: user 1h 27min 50s, sys: 23min 23s, total: 1h 51min 14s\n",
      "Wall time: 6min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8695218295218295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
